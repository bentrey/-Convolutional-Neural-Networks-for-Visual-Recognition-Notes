{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Softmax</b></p>\n",
    "<p>The softmax function takes as input a vector $z$ of $K$ real numbers, and normalizes it into a probability distribution consisting of $K$ probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval $(0,1)$, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.</p>\n",
    "$$ p_i = \\frac{e^{o_i}}{\\sum\\limits_{j} e^{o_j}}$$\n",
    "$$ y_i = \\frac{e^{m_i}}{\\sum\\limits_{j} e^{m_j}}$$\n",
    "$$L = -\\sum\\limits_{j} y_j \\ln{p_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_yi = np.exp(x - np.max(x))\n",
    "    return exp_yi/exp_yi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "x = np.array([1,2,3])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial p_i }{\\partial o_k} = \n",
    "\\frac{\\delta_{ik} e^{o_i} \\sum e^{o_i} - e^{o_i}e^{o_k} }{\\left(\\sum e^{o_i}\\right)^2}=\n",
    "\\delta_{ik} p_k - p_i p_k $$\n",
    "$$\\frac{\\partial L}{\\partial o_k} = -\\sum\\limits_{j} \n",
    " \\frac{y_j}{p_j}\\left(\\delta_{kj} p_k - p_j p_k \\right) = \n",
    "-y_k + \\sum\\limits_{j} y_j p_k =\n",
    "-y_k + p_k\\sum\\limits_{j} y_j =\n",
    "-y_k+p_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_gradient(o,m):\n",
    "    y = softmax(o)\n",
    "    p = softmax(m)\n",
    "    loss = -y.dot(np.log(p))\n",
    "    return {'gradient': -softmax(m)+softmax(o), 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradient': array([-0.03192108, -0.08677049,  0.11869157]),\n",
       " 'loss': 0.8615407006196977}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example \n",
    "o = np.array([1,2,3])\n",
    "m = np.array([1.5,2.5,3])\n",
    "softmax_loss_gradient(o,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
