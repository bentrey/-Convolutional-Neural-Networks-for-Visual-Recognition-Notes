{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Softmax</b></p>\n",
    "<p>The softmax function takes as input a vector $z$ of $K$ real numbers, and normalizes it into a probability distribution consisting of $K$ probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval $(0,1)$, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.</p>\n",
    "$$ p_i = \\frac{e^{o_i}}{\\sum\\limits_{j} e^{o_j}}$$\n",
    "$$ y_i = \\frac{e^{m_i}}{\\sum\\limits_{j} e^{m_j}}$$\n",
    "$$L = -\\sum\\limits_{j} y_j \\ln{p_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_yi = np.exp(x - np.max(x))\n",
    "    return exp_yi/exp_yi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "x = np.array([1,2,3])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial p_i }{\\partial o_k} = \n",
    "\\frac{\\delta_{ik} e^{o_i} \\sum e^{o_i} - e^{o_i}e^{o_k} }{\\left(\\sum e^{o_i}\\right)^2}=\n",
    "\\delta_{ik} p_k - p_i p_k $$\n",
    "$$\\frac{\\partial L}{\\partial o_k} = -\\sum\\limits_{j} \n",
    " \\frac{y_j}{p_j}\\left(\\delta_{kj} p_k - p_j p_k \\right) = \n",
    "-y_k + \\sum\\limits_{j} y_j p_k =\n",
    "-y_k + p_k\\sum\\limits_{j} y_j =\n",
    "-y_k+p_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_gradient(m,o):\n",
    "    y = softmax(o)\n",
    "    p = softmax(m)\n",
    "    loss = -y.dot(np.log(p))\n",
    "    return {'gradient': -y + p, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o value:  [2.01195234 4.03135564 7.95669202]  loss:  2.0782868945020683\n",
      "o value:  [3.04176524 5.2185721  5.73966266]  loss:  1.0103558587037593\n",
      "o value:  [3.6033989  4.95988652 5.43671458]  loss:  0.9587703901696916\n",
      "o value:  [3.78186451 4.86256822 5.35556727]  loss:  0.9531285143816612\n",
      "o value:  [3.82269681 4.83944202 5.33786117]  loss:  0.9528215051015863\n",
      "o value:  [3.83117366 4.83457725 5.33424909]  loss:  0.9528081408284297\n",
      "o value:  [3.8328964  4.83358516 5.33351844]  loss:  0.952807587640006\n",
      "o value:  [3.833245   4.83338425 5.33337075]  loss:  0.9528075649791481\n",
      "o value:  [3.83331548 4.83334363 5.3333409 ]  loss:  0.9528075640528291\n",
      "o value:  [3.83332972 4.83333541 5.33333486]  loss:  0.9528075640149797\n",
      "o value:  [3.8333326  4.83333375 5.33333364]  loss:  0.9528075640134333\n",
      "o value:  [3.83333319 4.83333342 5.3333334 ]  loss:  0.95280756401337\n",
      "o value:  [3.8333333  4.83333335 5.33333335]  loss:  0.9528075640133674\n",
      "o value:  [3.83333333 4.83333334 5.33333334]  loss:  0.9528075640133673\n",
      "o value:  [3.83333333 4.83333333 5.33333333]  loss:  0.9528075640133673\n",
      "o value:  [3.83333333 4.83333333 5.33333333]  loss:  0.9528075640133673\n",
      "o value:  [3.83333333 4.83333333 5.33333333]  loss:  0.9528075640133673\n",
      "o value:  [3.83333333 4.83333333 5.33333333]  loss:  0.9528075640133673\n",
      "o value:  [3.83333333 4.83333333 5.33333333]  loss:  0.9528075640133674\n",
      "o value:  [3.83333333 4.83333333 5.33333333]  loss:  0.9528075640133674\n"
     ]
    }
   ],
   "source": [
    "#Example \n",
    "o = np.array([2,4,8])\n",
    "m = np.array([1.5,2.5,3])\n",
    "steps = 2*10**3\n",
    "learning_rate = 0.1\n",
    "for n in range(steps):\n",
    "    grads = softmax_loss_gradient(o,m)\n",
    "    o = o - learning_rate*grads['gradient']\n",
    "    if n%100 == 0:\n",
    "        print(\"o value: \",o,\" loss: \",grads['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>K Nearest Neighbors</b></p>\n",
    "<p>In $k-NN$ classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its $k$ nearest neighbors ($k$ is a positive integer, typically small). If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor. In k-NN regression, the output is the property value for the object. This value is the average of the values of $k$ nearest neighbors.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d1(x1,x2):\n",
    "    x1 = x1.flatten()\n",
    "    x2 = x2.flatten()\n",
    "    diff = x2-x1\n",
    "    diff = abs(diff)\n",
    "    return diff.sum()\n",
    "\n",
    "def d2(x1,x2):\n",
    "    x1 = x1.flatten()\n",
    "    x2 = x2.flatten()\n",
    "    diff = x2-x1\n",
    "    diff_sqr = diff**2\n",
    "    return np.sqrt(diff_sqr.sum())\n",
    "\n",
    "def knn_classification(element, group, labels, k, l=2):\n",
    "    if l == 2:\n",
    "        d = d2\n",
    "    else:\n",
    "        d = d1\n",
    "    distances = [(d(element,group[n]), n) for n in range(len(labels))]\n",
    "    distances.sort()\n",
    "    distances = distances[:k]\n",
    "    classifications = [labels[distance[1]] for distance in distances]\n",
    "    return_classification = -1\n",
    "    for classification in classifications:\n",
    "        if classifications.count(classification) > return_classification:\n",
    "            return_classification = classification\n",
    "    return return_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 4, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = np.random.random((5))\n",
    "elements = np.random.random((50,5))\n",
    "labels = [ int(3*(element**2).sum()) for element in elements]\n",
    "knn_classification(element, elements, labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
